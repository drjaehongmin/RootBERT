{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Pre-Trainer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qSuUpkj1UuUa"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drjaehongmin/hello-world/blob/master/BERT_Pre_Trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNnAfKO0Y0OF",
        "outputId": "53e06f43-0287-4846-9de9-fa2175042bcf"
      },
      "source": [
        "# Jae-Hong Min\n",
        "# Last Edit 04 Aug 2021\n",
        "# Initially Based Off of: https://github.com/gmihaila/ml_things.git\n",
        "\n",
        "# Parameters Panel\n",
        "\n",
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "# Mounts the Google Drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#=================================#\n",
        "# Experimental Parameters         #\n",
        "#=================================#\n",
        "\n",
        "tokenizer_file_path = '/content/drive/MyDrive/Academic Research/W266 Final Project/test/'\n",
        "#bert_model_selected = '/content/drive/MyDrive/Academic Research/W266 Final Project/models/drBERT_small_v2'\n",
        "#tokenizer_file_path = \"bert-base-cased\"\n",
        "#bert_model_selected = \"bert-base-cased\"\n",
        "\n",
        "\n",
        "train_data_file_source='/content/drive/MyDrive/Academic Research/W266 Final Project/BERT_Training_Corpus/train.txt', \n",
        "eval_data_file_source ='/content/drive/MyDrive/Academic Research/W266 Final Project/BERT_Training_Corpus/test.txt', "
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25YWv_LmZEkW"
      },
      "source": [
        "# BERT Pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uod1B8gfZJBy"
      },
      "source": [
        "## Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JQhmThRXp7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cfea62-fd87-435c-e1dc-b84ec9aa123b"
      },
      "source": [
        "# Install transformers library.\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "# Install helper functions.\n",
        "!pip install -q git+https://github.com/gmihaila/ml_things.git"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux2sgMCvq8PE"
      },
      "source": [
        "# Import necessary libraries\n",
        "import io\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import warnings\n",
        "from tqdm.notebook import tqdm\n",
        "from ml_things import plot_dict, fix_text\n",
        "from transformers import (\n",
        "                          CONFIG_MAPPING,\n",
        "                          MODEL_FOR_MASKED_LM_MAPPING,\n",
        "                          MODEL_FOR_CAUSAL_LM_MAPPING,\n",
        "                          PreTrainedTokenizer,\n",
        "                          TrainingArguments,\n",
        "                          AutoConfig,\n",
        "                          AutoTokenizer,\n",
        "                          AutoModelWithLMHead,\n",
        "                          AutoModelForCausalLM,\n",
        "                          AutoModelForMaskedLM,\n",
        "                          LineByLineTextDataset,\n",
        "                          TextDataset,\n",
        "                          DataCollatorForLanguageModeling,\n",
        "                          DataCollatorForWholeWordMask,\n",
        "                          DataCollatorForPermutationLanguageModeling,\n",
        "                          PretrainedConfig,\n",
        "                          Trainer,\n",
        "                          set_seed,\n",
        "                          )\n",
        "\n",
        "# Set seed for reproducibility,\n",
        "set_seed(42)\n",
        "\n",
        "# Look for gpu to use. Will use `cpu` by default if no gpu found.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2DABVOmwC_J"
      },
      "source": [
        "\n",
        "class ModelDataArguments(object):\n",
        "   def __init__(self, train_data_file=None, eval_data_file=None, \n",
        "               line_by_line=False, mlm=False, mlm_probability=0.15, \n",
        "               whole_word_mask=False, plm_probability=float(1/6), \n",
        "               max_span_length=5, block_size=-1, overwrite_cache=False, \n",
        "               model_type=None, model_config_name=None, tokenizer_name=None, \n",
        "               model_name_or_path= None, model_cache_dir=None):\n",
        "    \n",
        "    # Make sure CONFIG_MAPPING is imported from transformers module.\n",
        "    if 'CONFIG_MAPPING' not in globals():\n",
        "      raise ValueError('Could not find `CONFIG_MAPPING` imported! Make sure' \\\n",
        "                       ' to import it from `transformers` module!')\n",
        "\n",
        "    # Make sure model_type is valid.\n",
        "    if (model_type is not None) and (model_type not in CONFIG_MAPPING.keys()):\n",
        "      raise ValueError('Invalid `model_type`! Use one of the following: %s' % \n",
        "                       (str(list(CONFIG_MAPPING.keys()))))\n",
        "      \n",
        "    # Make sure that model_type, model_config_name and model_name_or_path \n",
        "    # variables are not all `None`.\n",
        "    if not any([model_type, model_config_name, model_name_or_path]):\n",
        "      raise ValueError('You can`t have all `model_type`, `model_config_name`,' \\\n",
        "                       ' `model_name_or_path` be `None`! You need to have' \\\n",
        "                       'at least one of them set!')\n",
        "    \n",
        "    # Check if a new model will be loaded from scratch.\n",
        "    if not any([model_config_name, model_name_or_path]):\n",
        "      # Setup warning to show pretty. This is an overkill\n",
        "      warnings.formatwarning = lambda message,category,*args,**kwargs: \\\n",
        "                               '%s: %s\\n' % (category.__name__, message)\n",
        "      # Display warning.\n",
        "      warnings.warn('Check Inputs\")\n",
        "\n",
        "    # Check if a new tokenizer wants to be loaded.\n",
        "    # This feature is not supported!\n",
        "    if not any([tokenizer_name, model_name_or_path]):\n",
        "      # Can't train tokenizer from scratch here! Raise error.\n",
        "      raise ValueError('Check Inputs\")\n",
        "\n",
        "      \n",
        "    # Set all data related arguments.\n",
        "    self.train_data_file = train_data_file\n",
        "    self.eval_data_file = eval_data_file\n",
        "    self.line_by_line = line_by_line\n",
        "    self.mlm = mlm\n",
        "    self.whole_word_mask = whole_word_mask\n",
        "    self.mlm_probability = mlm_probability\n",
        "    self.plm_probability = plm_probability\n",
        "    self.max_span_length = max_span_length\n",
        "    self.block_size = block_size\n",
        "    self.overwrite_cache = overwrite_cache\n",
        "\n",
        "    # Set all model and tokenizer arguments.\n",
        "    self.model_type = model_type\n",
        "    self.model_config_name = model_config_name\n",
        "    self.tokenizer_name = tokenizer_name\n",
        "    self.model_name_or_path = model_name_or_path\n",
        "    self.model_cache_dir = model_cache_dir\n",
        "    return\n",
        "\n",
        "\n",
        "def get_model_config(args: ModelDataArguments):\n",
        "  # Check model configuration.\n",
        "  if args.model_config_name is not None:\n",
        "    # Use model configure name if defined.\n",
        "    model_config = AutoConfig.from_pretrained(args.model_config_name, \n",
        "                                      cache_dir=args.model_cache_dir)\n",
        "\n",
        "  elif args.model_name_or_path is not None:\n",
        "    # Use model name or path if defined.\n",
        "    model_config = AutoConfig.from_pretrained(args.model_name_or_path, \n",
        "                                      cache_dir=args.model_cache_dir)\n",
        "\n",
        "  else:\n",
        "    # Use config mapping if building model from scratch.\n",
        "    model_config = CONFIG_MAPPING[args.model_type]()\n",
        "\n",
        "  # Make sure `mlm` flag is set for Masked Language Models (MLM).\n",
        "  if (model_config.model_type in [\"bert\", \"roberta\", \"distilbert\", \n",
        "                                  \"camembert\"]) and (args.mlm is False):\n",
        "    raise ValueError('BERT and RoBERTa-like models do not have LM heads ' \\\n",
        "                    'butmasked LM heads. They must be run setting `mlm=True`')\n",
        "  \n",
        "  # Adjust block size for xlnet.\n",
        "  if model_config.model_type == \"xlnet\":\n",
        "    # xlnet used 512 tokens when training.\n",
        "    args.block_size = 512\n",
        "    # setup memory length\n",
        "    model_config.mem_len = 1024\n",
        "  \n",
        "  return model_config\n",
        "\n",
        "\n",
        "def get_tokenizer(args: ModelDataArguments):\n",
        "  # Check model configuration.\n",
        "  if args.model_config_name is not None:\n",
        "    # Use model configure name if defined.\n",
        "    model_config = AutoConfig.from_pretrained(args.model_config_name, \n",
        "                                      cache_dir=args.model_cache_dir)\n",
        "\n",
        "  elif args.model_name_or_path is not None:\n",
        "    # Use model name or path if defined.\n",
        "    model_config = AutoConfig.from_pretrained(args.model_name_or_path, \n",
        "                                      cache_dir=args.model_cache_dir)\n",
        "    \n",
        "  tokenizer = BertTokenizer.from_pretrained(tokenizer_file_path)\n",
        "    \n",
        "  # Setp data block size.\n",
        "  if args.block_size <= 0:\n",
        "    # Set block size to maximum length of tokenizer.\n",
        "    # Input block size will be the max possible for the model.\n",
        "    # Some max lengths are very large and will cause a\n",
        "    args.block_size = tokenizer.model_max_length\n",
        "  else:\n",
        "    # Never go beyond tokenizer maximum length.\n",
        "    args.block_size = min(args.block_size, tokenizer.model_max_length)\n",
        "\n",
        "  return tokenizer\n",
        "  \n",
        "\n",
        "def get_model(args: ModelDataArguments, model_config):\n",
        "  # Make sure MODEL_FOR_MASKED_LM_MAPPING and MODEL_FOR_CAUSAL_LM_MAPPING are \n",
        "  # imported from transformers module.\n",
        "  if ('MODEL_FOR_MASKED_LM_MAPPING' not in globals()) and \\\n",
        "                ('MODEL_FOR_CAUSAL_LM_MAPPING' not in globals()):\n",
        "    raise ValueError('Could not find `MODEL_FOR_MASKED_LM_MAPPING` and' \\\n",
        "                     ' `MODEL_FOR_MASKED_LM_MAPPING` imported! Make sure to' \\\n",
        "                     ' import them from `transformers` module!')\n",
        "    \n",
        "  # Check if using pre-trained model or train from scratch.\n",
        "  if args.model_name_or_path:\n",
        "    # Use pre-trained model.\n",
        "    if type(model_config) in MODEL_FOR_MASKED_LM_MAPPING.keys():\n",
        "      # Masked language modeling head.\n",
        "      return AutoModelForMaskedLM.from_pretrained(\n",
        "                        args.model_name_or_path,\n",
        "                        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n",
        "                        config=model_config,\n",
        "                        cache_dir=args.model_cache_dir,\n",
        "                        )\n",
        "    elif type(model_config) in MODEL_FOR_CAUSAL_LM_MAPPING.keys():\n",
        "      # Causal language modeling head.\n",
        "      return AutoModelForCausalLM.from_pretrained(\n",
        "                                          args.model_name_or_path, \n",
        "                                          from_tf=bool(\".ckpt\" in \n",
        "                                                        args.model_name_or_path),\n",
        "                                          config=model_config, \n",
        "                                          cache_dir=args.model_cache_dir)\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          'Invalid `model_name_or_path`! It should be in %s or %s!' % \n",
        "          (str(MODEL_FOR_MASKED_LM_MAPPING.keys()), \n",
        "           str(MODEL_FOR_CAUSAL_LM_MAPPING.keys())))\n",
        "    \n",
        "  else:\n",
        "    # Use model from configuration - train from scratch.\n",
        "      print(\"Training new model from scratch!\")\n",
        "      return AutoModelWithLMHead.from_config(config)\n",
        "\n",
        "\n",
        "def get_dataset(args: ModelDataArguments, tokenizer: PreTrainedTokenizer, \n",
        "                evaluate: bool=False):\n",
        "  # Get file path for either train or evaluate.\n",
        "  file_path = args.eval_data_file if evaluate else args.train_data_file\n",
        "\n",
        "  # Check if `line_by_line` flag is set to `True`.\n",
        "  if args.line_by_line:\n",
        "    # Each example in data file is on each line.\n",
        "    return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, \n",
        "                                 block_size=args.block_size)\n",
        "    \n",
        "  else:\n",
        "    # All data in file is put together without any separation.\n",
        "    return TextDataset(tokenizer=tokenizer, file_path=file_path, \n",
        "                       block_size=args.block_size, \n",
        "                       overwrite_cache=args.overwrite_cache)\n",
        "\n",
        "\n",
        "def get_collator(args: ModelDataArguments, model_config: PretrainedConfig, \n",
        "                 tokenizer: PreTrainedTokenizer):\n",
        "  # Special dataset handle depending on model type.\n",
        "  if model_config.model_type == \"xlnet\":\n",
        "    # Configure collator for XLNET.\n",
        "    return DataCollatorForPermutationLanguageModeling(\n",
        "                                          tokenizer=tokenizer,\n",
        "                                          plm_probability=args.plm_probability,\n",
        "                                          max_span_length=args.max_span_length,\n",
        "                                          )\n",
        "  else:\n",
        "    # Configure data for rest of model types.\n",
        "    if args.mlm and args.whole_word_mask:\n",
        "      # Use whole word masking.\n",
        "      return DataCollatorForWholeWordMask(\n",
        "                                          tokenizer=tokenizer, \n",
        "                                          mlm_probability=args.mlm_probability,\n",
        "                                          )\n",
        "    else:\n",
        "      # Regular language modeling.\n",
        "      return DataCollatorForLanguageModeling(\n",
        "                                          tokenizer=tokenizer, \n",
        "                                          mlm=args.mlm, \n",
        "                                          mlm_probability=args.mlm_probability,\n",
        "                                          )\n"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LACnz9HkdQJS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18eb35a1-43f4-43b0-c3a1-9eeb824d33a8"
      },
      "source": [
        "# Define arguments for data, tokenizer and model arguments.\n",
        "# See comments in `ModelDataArguments` class.\n",
        "model_data_args = ModelDataArguments(\n",
        "                                    train_data_file='/content/drive/MyDrive/Academic Research/W266 Final Project/BERT_Training_Corpus/train.txt', \n",
        "                                    eval_data_file='/content/drive/MyDrive/Academic Research/W266 Final Project/BERT_Training_Corpus/test.txt', \n",
        "                                    line_by_line=True, \n",
        "                                    mlm=True,\n",
        "                                    whole_word_mask=True,\n",
        "                                    mlm_probability=0.15,\n",
        "                                    plm_probability=float(1/6), \n",
        "                                    max_span_length=5,\n",
        "                                    block_size=50, \n",
        "                                    overwrite_cache=False, \n",
        "                                    model_type='bert', \n",
        "                                    model_config_name='bert-base-cased', \n",
        "                                    #tokenizer_name='bert-base-cased', \n",
        "                                    tokenizer_name = \"root\",\n",
        "                                    #model_name_or_path='bert-base-cased',\n",
        "                                    model_name_or_path = None,\n",
        "                                    model_cache_dir=None,\n",
        "                                    )\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "                          # The output directory where the model predictions \n",
        "                          # and checkpoints will be written.\n",
        "                          output_dir='pretrain_bert',\n",
        "\n",
        "                          # Overwrite the content of the output directory.\n",
        "                          overwrite_output_dir=True,\n",
        "\n",
        "                          # Whether to run training or not.\n",
        "                          do_train=True, \n",
        "                          \n",
        "                          # Whether to run evaluation on the dev or not.\n",
        "                          do_eval=True,\n",
        "                          \n",
        "                          # Batch size GPU/TPU core/CPU training.\n",
        "                          per_device_train_batch_size=10,\n",
        "                          \n",
        "                          # Batch size  GPU/TPU core/CPU for evaluation.\n",
        "                          per_device_eval_batch_size=100,\n",
        "\n",
        "                          # evaluation strategy to adopt during training\n",
        "                          # `no`: No evaluation during training.\n",
        "                          # `steps`: Evaluate every `eval_steps`.\n",
        "                          # `epoch`: Evaluate every end of epoch.\n",
        "                          evaluation_strategy='steps',\n",
        "\n",
        "                          # How often to show logs. I will se this to \n",
        "                          # plot history loss and calculate perplexity.\n",
        "                          logging_steps=700,\n",
        "\n",
        "                          # Number of update steps between two \n",
        "                          # evaluations if evaluation_strategy=\"steps\".\n",
        "                          # Will default to the same value as l\n",
        "                          # logging_steps if not set.\n",
        "                          eval_steps = None,\n",
        "                          \n",
        "                          # Set prediction loss to `True` in order to \n",
        "                          # return loss for perplexity calculation.\n",
        "                          prediction_loss_only=True,\n",
        "\n",
        "                          # The initial learning rate for Adam. \n",
        "                          # Defaults to 5e-5.\n",
        "                          learning_rate = 5e-5,\n",
        "\n",
        "                          # The weight decay to apply (if not zero).\n",
        "                          weight_decay=0,\n",
        "\n",
        "                          # Epsilon for the Adam optimizer. \n",
        "                          # Defaults to 1e-8\n",
        "                          adam_epsilon = 1e-8,\n",
        "\n",
        "                          # Maximum gradient norm (for gradient \n",
        "                          # clipping). Defaults to 0.\n",
        "                          max_grad_norm = 1.0,\n",
        "                          # Total number of training epochs to perform \n",
        "                          # (if not an integer, will perform the \n",
        "                          # decimal part percents of\n",
        "                          # the last epoch before stopping training).\n",
        "                          num_train_epochs = 2,\n",
        "\n",
        "                          # Number of updates steps before two checkpoint saves. \n",
        "                          # Defaults to 500\n",
        "                          save_steps = -1,\n",
        "                          )"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using `logging_steps` to initialize `eval_steps` to 700\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyUXB374duJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc54ff2c-b3fc-4ddb-eeff-f31d823a0626"
      },
      "source": [
        "# Load model configuration.\n",
        "print('Loading model configuration...')\n",
        "config = get_model_config(model_data_args)\n",
        "\n",
        "# Load model tokenizer.\n",
        "print('Loading model`s tokenizer...')\n",
        "tokenizer = get_tokenizer(model_data_args)\n",
        "\n",
        "# Loading model.\n",
        "print('Loading actual model...')\n",
        "model = get_model(model_data_args, config)\n",
        "\n",
        "# Resize model to fit all tokens in tokenizer.\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model configuration...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading model`s tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.10.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "Didn't find file /content/drive/MyDrive/Academic Research/W266 Final Project/test/added_tokens.json. We won't load it.\n",
            "Didn't find file /content/drive/MyDrive/Academic Research/W266 Final Project/test/tokenizer.json. We won't load it.\n",
            "loading file /content/drive/MyDrive/Academic Research/W266 Final Project/test/vocab.txt\n",
            "loading file None\n",
            "loading file /content/drive/MyDrive/Academic Research/W266 Final Project/test/special_tokens_map.json\n",
            "loading file /content/drive/MyDrive/Academic Research/W266 Final Project/test/tokenizer_config.json\n",
            "loading file None\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:914: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading actual model...\n",
            "Training new model from scratch!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30522, 768)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mkU59HSd77l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5471fed3-bf32-4d88-8fe6-b952b5814048"
      },
      "source": [
        "# Setup train dataset if `do_train` is set.\n",
        "print('Creating train dataset...')\n",
        "train_dataset = get_dataset(model_data_args, tokenizer=tokenizer, evaluate=False) if training_args.do_train else None\n",
        "\n",
        "# Setup evaluation dataset if `do_eval` is set.\n",
        "print('Creating evaluate dataset...')\n",
        "eval_dataset = get_dataset(model_data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None\n",
        "\n",
        "# Get data collator to modify data format depending on type of model used.\n",
        "data_collator = get_collator(model_data_args, config, tokenizer)\n",
        "\n",
        "# Check how many logging prints you'll have. This is to avoid overflowing the \n",
        "# notebook with a lot of prints. Display warning to user if the logging steps \n",
        "# that will be displayed is larger than 100.\n",
        "if (len(train_dataset) // training_args.per_device_train_batch_size \\\n",
        "    // training_args.logging_steps * training_args.num_train_epochs) > 100:\n",
        "  # Display warning.\n",
        "  warnings.warn('Your `logging_steps` value will will do a lot of printing!' \\\n",
        "                ' Consider increasing `logging_steps` to avoid overflowing' \\\n",
        "                ' the notebook with a lot of prints!')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Creating features from dataset file at /content/drive/MyDrive/Academic Research/W266 Final Project/BERT_Training_Corpus/train.txt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating train dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating features from dataset file at /content/drive/MyDrive/Academic Research/W266 Final Project/BERT_Training_Corpus/test.txt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating evaluate dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: Your `logging_steps` value will will do a lot of printing! Consider increasing `logging_steps` to avoid overflowing the notebook with a lot of prints!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5R9DkijbbRg"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWZi6QArWR8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "55cf3351-f555-4db3-e5cc-c0de52965ea3"
      },
      "source": [
        "# Initialize Trainer.\n",
        "print('Loading `trainer`...')\n",
        "trainer = Trainer(model=model,\n",
        "                  args=training_args,\n",
        "                  data_collator=data_collator,\n",
        "                  train_dataset=train_dataset,\n",
        "                  eval_dataset=eval_dataset,\n",
        "                  )\n",
        "\n",
        "\n",
        "# Check model path to save.\n",
        "if training_args.do_train:\n",
        "  print('Start training...')\n",
        "\n",
        "  # Setup model path if the model to train loaded from a local path.\n",
        "  model_path = (model_data_args.model_name_or_path \n",
        "                if model_data_args.model_name_or_path is not None and \n",
        "                os.path.isdir(model_data_args.model_name_or_path) \n",
        "                else None\n",
        "                )\n",
        "  # Run training.\n",
        "  trainer.train(model_path=model_path)\n",
        "  # Save model.\n",
        "  trainer.save_model()\n",
        "\n",
        "  # For convenience, we also re-save the tokenizer to the same directory,\n",
        "  # so that you can share your model easily on huggingface.co/models =).\n",
        "  if trainer.is_world_process_zero():\n",
        "    tokenizer.save_pretrained(training_args.output_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading `trainer`...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1027: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1858362\n",
            "  Num Epochs = 2\n",
            "  Instantaneous batch size per device = 10\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 371674\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Start training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16801' max='371674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 16801/371674 2:58:02 < 62:41:12, 1.57 it/s, Epoch 0.09/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>7.155100</td>\n",
              "      <td>6.792943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>6.720900</td>\n",
              "      <td>6.614862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>6.582900</td>\n",
              "      <td>6.507611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>6.501500</td>\n",
              "      <td>6.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>6.454600</td>\n",
              "      <td>6.395853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>6.341500</td>\n",
              "      <td>6.288786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>6.227300</td>\n",
              "      <td>6.130356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>6.123100</td>\n",
              "      <td>5.998371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>5.989700</td>\n",
              "      <td>5.867303</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>5.865200</td>\n",
              "      <td>5.701867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>5.669500</td>\n",
              "      <td>5.493716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>5.491000</td>\n",
              "      <td>5.359750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9100</td>\n",
              "      <td>5.408200</td>\n",
              "      <td>5.269511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9800</td>\n",
              "      <td>5.286200</td>\n",
              "      <td>5.158301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>5.180900</td>\n",
              "      <td>5.090939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11200</td>\n",
              "      <td>5.139200</td>\n",
              "      <td>5.012060</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11900</td>\n",
              "      <td>5.037300</td>\n",
              "      <td>4.944384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12600</td>\n",
              "      <td>4.986000</td>\n",
              "      <td>4.888956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13300</td>\n",
              "      <td>4.920400</td>\n",
              "      <td>4.840809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>4.879900</td>\n",
              "      <td>4.781548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14700</td>\n",
              "      <td>4.799900</td>\n",
              "      <td>4.718828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15400</td>\n",
              "      <td>4.766800</td>\n",
              "      <td>4.674026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16100</td>\n",
              "      <td>4.746900</td>\n",
              "      <td>4.638553</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='491' max='2065' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 491/2065 01:32 < 04:57, 5.29 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 206486\n",
            "  Batch size = 100\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FEY2WDgmCD_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "98b0354b-1085-41b9-9e1d-0ec6d1013506"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Keep track of train and evaluate loss.\n",
        "loss_history = {'train_loss':[], 'eval_loss':[]}\n",
        "\n",
        "# Keep track of train and evaluate perplexity.\n",
        "# This is a metric useful to track for language models.\n",
        "perplexity_history = {'train_perplexity':[], 'eval_perplexity':[]}\n",
        "\n",
        "# Loop through each log history.\n",
        "for log_history in trainer.state.log_history:\n",
        "\n",
        "  if 'loss' in log_history.keys():\n",
        "    # Deal with trianing loss.\n",
        "    loss_history['train_loss'].append(log_history['loss'])\n",
        "    perplexity_history['train_perplexity'].append(math.exp(log_history['loss']))\n",
        "    \n",
        "  elif 'eval_loss' in log_history.keys():\n",
        "    # Deal with eval loss.\n",
        "    loss_history['eval_loss'].append(log_history['eval_loss'])\n",
        "    perplexity_history['eval_perplexity'].append(math.exp(log_history['eval_loss']))\n",
        "\n",
        "# Plot Losses.\n",
        "plot_dict(loss_history, start_step=training_args.logging_steps, \n",
        "          step_size=training_args.logging_steps, use_title='Loss', \n",
        "          use_xlabel='Train Steps', use_ylabel='Values', magnify=2)\n",
        "\n",
        "print()\n",
        "\n",
        "# Plot Perplexities.\n",
        "plot_dict(perplexity_history, start_step=training_args.logging_steps, \n",
        "          step_size=training_args.logging_steps, use_title='Perplexity', \n",
        "          use_xlabel='Train Steps', use_ylabel='Values', magnify=2)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ml_things/plot_functions.py:410: DeprecationWarning: `magnify` needs to have value in [0,1]! `2` will be converted to `0.1` as default.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0mfmt\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mdetermine\u001b[0m \u001b[0ma\u001b[0m \u001b[0msuitable\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m             \u001b[0msaving\u001b[0m \u001b[0mto\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0meither\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m             \u001b[0msupports\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwhatever\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_registered_canvas_class\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m             \u001b[0mswitch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfigure\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m_get_renderer\u001b[0;34m(figure, print_method)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \"\"\"\n\u001b[1;32m   1559\u001b[0m     \u001b[0;31m# This is implemented by triggering a draw, then immediately jumping out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m     \u001b[0;31m# Figure.draw() by raising an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mKeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m'pnginfo'\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mpresent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcompletely\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \"\"\"\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_png' from 'matplotlib' (/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1944x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ml_things/plot_functions.py:410: DeprecationWarning: `magnify` needs to have value in [0,1]! `2` will be converted to `0.1` as default.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    332\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mprinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0;31m# Finally look for special method names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(fig)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'png'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'retina'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'png2x'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mformats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0mpng_formatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretina_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mbytes_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytes_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfmt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'svg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2092\u001b[0m         \u001b[0mfmt\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m             \u001b[0mIf\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mdetermine\u001b[0m \u001b[0ma\u001b[0m \u001b[0msuitable\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2094\u001b[0;31m             \u001b[0msaving\u001b[0m \u001b[0mto\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0meither\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2095\u001b[0m             \u001b[0msupports\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mwhatever\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mget_registered_canvas_class\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mreturns\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2096\u001b[0m             \u001b[0mswitch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfigure\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mcanvas\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m_get_renderer\u001b[0;34m(figure, print_method)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \"\"\"\n\u001b[1;32m   1559\u001b[0m     \u001b[0;31m# This is implemented by triggering a draw, then immediately jumping out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1560\u001b[0;31m     \u001b[0;31m# Figure.draw() by raising an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mDone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mKeyword\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m'pnginfo'\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mpresent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0mcompletely\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \"\"\"\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name '_png' from 'matplotlib' (/usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py)"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1944x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vzFHOkLXKAp"
      },
      "source": [
        "## **Evaluate**\n",
        "\n",
        "For the final evaluation we can have a separate test set that we use to do our final perplexity evaluation. For simplicity I used the same validation text file for the final evaluation. That is the reason I get the same results as the last validation perplexity plot value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHS0IWmpXLA6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f7720a0c-cc08-4585-f294-69ab8ef53ebe"
      },
      "source": [
        "# check if `do_eval` flag is set.\n",
        "if training_args.do_eval:\n",
        "  \n",
        "  # capture output if trainer evaluate.\n",
        "  eval_output = trainer.evaluate()\n",
        "  # compute perplexity from model loss.\n",
        "  perplexity = math.exp(eval_output[\"eval_loss\"])\n",
        "  print('\\nEvaluate Perplexity: {:10,.2f}'.format(perplexity))\n",
        "else:\n",
        "  print('No evaluation needed. No evaluation data provided, `do_eval=False`!')"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 5001\n",
            "  Batch size = 100\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 00:09]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluate Perplexity:     538.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DExFyvBVOy3w",
        "outputId": "b25c432d-18bf-4dc5-b1f8-1d45c7881fb3"
      },
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Academic Research/W266 Final Project/models/drBERT_v2\")"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Configuration saved in /content/drive/MyDrive/Academic Research/W266 Final Project/models/drBERT_small_v2/config.json\n",
            "Model weights saved in /content/drive/MyDrive/Academic Research/W266 Final Project/models/drBERT_small_v2/pytorch_model.bin\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}