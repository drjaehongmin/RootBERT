{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_tokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "isGMmQbXsBfC",
        "G0tZcFNns2eB",
        "kX0DB5Y6yV15",
        "2-K-xHKq3bwO",
        "1k7d4RVrtfOf"
      ],
      "authorship_tag": "ABX9TyNCptOn686sQne5J/BHH8eX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drjaehongmin/hello-world/blob/master/Train_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMiqnGEsrPfA"
      },
      "source": [
        "# Creates Tokenizer and also datasets / Corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGTxYJlZraIq"
      },
      "source": [
        "## Initialize Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8HDvxsErZba",
        "outputId": "3d86f76b-664d-46c6-d48d-c8140302f141"
      },
      "source": [
        "# Install required packages\n",
        "! pip install tokenizers===0.9.3"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tokenizers===0.9.3\n",
            "  Downloading tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 7.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: tokenizers\n",
            "Successfully installed tokenizers-0.9.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWsJTADNrUBE"
      },
      "source": [
        "# Load required libraries\n",
        "import pandas as pd\n",
        "from lxml import etree\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from random import randint\n",
        "import random\n",
        "random.seed(42)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8ZaubLArN-1",
        "outputId": "8529de9b-949e-44ff-b500-6392c6389214"
      },
      "source": [
        "# Mounts Google Drive with data\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import glob\n",
        "\n",
        "# Mounts the Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isGMmQbXsBfC"
      },
      "source": [
        "## Creates Root Word Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfRd3v4_sGqv"
      },
      "source": [
        "root_corpus = []\n",
        "with open(\"/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/Root_words_v2.csv\") as root_words:\n",
        "  root_corpus.append(root_words.readlines())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDYCsUrLsKgZ"
      },
      "source": [
        "clean_roots = []\n",
        "\n",
        "for x in root_corpus[0]:\n",
        "  test = str(x)\n",
        "  splits = test.split(\",\")\n",
        "  for y in splits:\n",
        "    new = re.sub('#NAME?', '', y)\n",
        "    clean_roots.append(new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dYRZX6ssNBi"
      },
      "source": [
        "clean_roots1 = []\n",
        "for x in clean_roots:\n",
        "  new = x.replace(\"\\n\", \"\") \n",
        "  new = new.replace(\"\\xa0\", \"\")\n",
        "  new = new.replace('\\\\\"', \"\")\n",
        "  new = new.replace(\"[edit]\", \"\")\n",
        "  new = new.replace('\"', \"\")\n",
        "  new = new.replace('singular', \"\")\n",
        "  new = new.replace('plural', \"\")\n",
        "  new = new.replace(' ', \"\")\n",
        "  new = re.sub('\\([A-Za-z0-9_]{1,3}\\)', '', new)\n",
        "  new = re.sub('[^A-Za-z]+', '', new)\n",
        "  splits = new.split(\"-\")\n",
        "  for y in splits:\n",
        "    clean_roots1.append(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzyr_i0OsPXu"
      },
      "source": [
        "clean_roots2 = []\n",
        "for new in clean_roots1:\n",
        "  new = new.replace('GreekrootinEnglish', \"\")\n",
        "  new = new.replace('oforpertainingtotheeyeliduncommonasarootscoperowpalpebr', \"\")\n",
        "  new = new.replace('LatinrootinEnglish', \"\")\n",
        "  clean_roots2.append(new)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXKZSuXdsRMV"
      },
      "source": [
        "clean_roots3 = []\n",
        "for x in clean_roots2:\n",
        "  if x in clean_roots3:\n",
        "    None\n",
        "  else:\n",
        "    clean_roots3.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaF0VcKpsSc2"
      },
      "source": [
        "clean_roots4 = [x for x in clean_roots3 if len(x) > 2]\n",
        "\n",
        "root_word_corpus_out = clean_roots4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzWV1AGqsWD5"
      },
      "source": [
        "vowel = {0:\"o\", 1:\"\"}\n",
        "root_word_corpus_created = []\n",
        "for x in range(0,5000000):\n",
        "  new_word = clean_roots4[randint(0, len(clean_roots4)-1)]\n",
        "  for y in range(0, randint(1, 3)):\n",
        "    new_word = new_word + vowel[randint(0,1)] + clean_roots4[randint(0, len(clean_roots4)-1)]\n",
        "  \n",
        "  root_word_corpus_created.append(new_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0tZcFNns2eB"
      },
      "source": [
        "## Creates USMC corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHBpn3ges2NI"
      },
      "source": [
        "# USMC data\n",
        "tree = etree.parse('/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/pa2021.xml')\n",
        "notags1 = etree.tostring(tree, encoding='utf8', method='text')\n",
        "text1 = notags1.decode(\"utf-8\") \n",
        "mod_string = re.sub('\\\\n', '', text1)\n",
        "\n",
        "tree = etree.parse('/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/desc2021.xml')\n",
        "notags1 = etree.tostring(tree, encoding='utf8', method='text')\n",
        "text2 = notags1.decode(\"utf-8\") \n",
        "mod_string2 = re.sub('\\\\n', '', text1)\n",
        "\n",
        "new_string3 = str(mod_string2) + str(mod_string)\n",
        "\n",
        "list_words = new_string3.split(\" \")\n",
        "len(list_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2oW-s71vh3C"
      },
      "source": [
        "usmc_list = []\n",
        "\n",
        "for x in list_words:\n",
        "  new = re.sub('[\\W0-9{4,7}]', '', x)\n",
        "  usmc_list.append(new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqaytn8NwSSg"
      },
      "source": [
        "usmc_list1 = [x for x in usmc_list if x != \"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDrJVowuxpzZ"
      },
      "source": [
        "usmc_list2 = [x for x in usmc_list1 if len(x) > 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feLfsqkhx1pY"
      },
      "source": [
        "usmc_list_out = usmc_list2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kX0DB5Y6yV15"
      },
      "source": [
        "## Creates Definition Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_u-R5Q7Myfqo"
      },
      "source": [
        "# Gets all the files from the corpus folder that have the *.gz extension\n",
        "filenames = glob.glob(\"/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/Definition Corpus/*.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7rhWnP2zL7v",
        "outputId": "4a9281ad-a90f-46dd-e71e-cf914735fd33"
      },
      "source": [
        "# Remainder of tokenizer corpus\n",
        "token_corpus = []\n",
        "\n",
        "for filename in filenames:\n",
        "  with open(filename) as f:\n",
        "      token_corpus.append(f.readlines())\n",
        "\n",
        "new_string = \" \".join(str(item) for item in token_corpus)\n",
        "len(new_string)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7199698"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wt9k8uDnz_CM",
        "outputId": "82e3f388-f051-4899-94c6-0f983f3bdb7e"
      },
      "source": [
        "split = new_string.split(\" \")\n",
        "print(len(split))\n",
        "\n",
        "definition_list = []\n",
        "\n",
        "for x in split:\n",
        "  new = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)', '', x)\n",
        "  new = new.replace('\\\\n', \"\")\n",
        "  new = re.sub('[^A-Za-z]', '', new)\n",
        "  definition_list.append(new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1136510\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4nM4YDL0nU4"
      },
      "source": [
        "definition_list1 = [x for x in definition_list if len(x) > 2]\n",
        "definition_out = definition_list1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpzcHg6V3GYP"
      },
      "source": [
        "## Load custom corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n2T9Zwk3KUu",
        "outputId": "3ea5b29d-17c7-4683-800d-42ac6af89d71"
      },
      "source": [
        "# Load corpus\n",
        "corpus_data = pd.read_csv(\"/content/drive/MyDrive/Academic Research/W266 Final Project/Corpus.csv\")\n",
        "corpus_data = corpus_data.set_index(\"Unnamed: 0\")\n",
        "print(\"Number of rows in the corpus is: \", len(corpus_data.index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows in the corpus is:  2064846\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahFr7IAn4bq9"
      },
      "source": [
        "# Convert Corpus data into list\n",
        "corpus_sentence = []\n",
        "\n",
        "for x in corpus_data.iloc[:,0]:\n",
        "  corpus_sentence.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDtCzjRm32t9"
      },
      "source": [
        "corpus_train, corpus_test = train_test_split(corpus_sentence, random_state=42, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfHDAsvp75YL"
      },
      "source": [
        "dict = {\"Corpus Train\": corpus_train[0:50000]}\n",
        "\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv(\"/content/drive/MyDrive/Academic Research/W266 Final Project/train_med.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqfDI6H4FRSs"
      },
      "source": [
        "dict = {\"Corpus Test\": corpus_test[0:10000]}\n",
        "\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv(\"/content/drive/MyDrive/Academic Research/W266 Final Project/test_med.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN200t1FFTXH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-K-xHKq3bwO"
      },
      "source": [
        "## Combines above and creates dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJb6CsXU3e72"
      },
      "source": [
        "total_corpus = []\n",
        "\n",
        "for x in root_word_corpus_created:\n",
        "  total_corpus.append(x)\n",
        "\n",
        "for x in root_word_corpus_out:\n",
        "  total_corpus.append(x)\n",
        "\n",
        "for x in usmc_list_out:\n",
        "  total_corpus.append(x)\n",
        "\n",
        "for x in definition_out:\n",
        "  total_corpus.append(x)\n",
        "\n",
        "for x in corpus_sentence:\n",
        "  total_corpus.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVbQYt565sCX"
      },
      "source": [
        "# Split Corpus into training, validation and testing datasets\n",
        "train_ratio = 0.95\n",
        "validation_ratio = 0.03\n",
        "test_ratio = 0.02\n",
        "\n",
        "x_train, x_test = train_test_split(total_corpus, test_size= (1 - train_ratio), random_state=42)\n",
        "x_val, x_test = train_test_split(x_test, test_size=test_ratio/(test_ratio + validation_ratio)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxSSnJ_l6osk"
      },
      "source": [
        "df_test = pd.DataFrame(x_test)\n",
        "df_train = pd.DataFrame(x_train)\n",
        "df_val = pd.DataFrame(x_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHidOKXc6qSr"
      },
      "source": [
        "df_test.to_csv(\"/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/Token_corpus.test.csv\")\n",
        "df_train.to_csv(\"/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/Token_corpus.train.csv\")\n",
        "df_val.to_csv(\"/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/Token_corpus.valid.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riv8lnO167Un"
      },
      "source": [
        "len(total_corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k7d4RVrtfOf"
      },
      "source": [
        "## Creates Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUYpYZ14thCy"
      },
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "\n",
        "bert_tokenizer = Tokenizer(WordPiece())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPkC0Cq7tj9l"
      },
      "source": [
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
        "\n",
        "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5NceKDXtlF1"
      },
      "source": [
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "bert_tokenizer.pre_tokenizer = Whitespace()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOQL0x0ntmRp"
      },
      "source": [
        "from tokenizers.processors import TemplateProcessing\n",
        "\n",
        "bert_tokenizer.post_processor = TemplateProcessing(\n",
        "    single=\"[CLS] $A [SEP]\",\n",
        "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
        "    special_tokens=[\n",
        "        (\"[CLS]\", 1),\n",
        "        (\"[SEP]\", 2),\n",
        "    ],\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q8pzv1RtnoT"
      },
      "source": [
        "from tokenizers.trainers import WordPieceTrainer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5IPRAbitowv"
      },
      "source": [
        "trainer = WordPieceTrainer(\n",
        "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAXQEUhVt1lY"
      },
      "source": [
        "files = [f\"/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/Token_corpus.{split}.csv\" for split in [\"test\", \"train\", \"valid\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dT8JJntO7VUc"
      },
      "source": [
        "bert_tokenizer.train(trainer, files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAWVb52Bt1-C"
      },
      "source": [
        "model_files = bert_tokenizer.model.save(\"\", \"custom_token\")\n",
        "bert_tokenizer.model = WordPiece.from_file(*model_files, unk_token=\"[UNK]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgr256a0t7Nh"
      },
      "source": [
        "bert_tokenizer.save(\"/content/drive/MyDrive/Academic Research/W266 Final Project/Token Data/bert_root.json\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "WpaFnIm19I5w",
        "outputId": "8aa008e0-7ff1-4cf4-e544-fef9027a51bf"
      },
      "source": [
        "bert_tokenizer.encode(\"Hello\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-ef83df3b0ab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hello\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mException\u001b[0m: WordPiece error: Missing [UNK] token from the vocabulary"
          ]
        }
      ]
    }
  ]
}